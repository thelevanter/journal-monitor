"""
ğŸ“š Journal Monitor Dashboard
ì¼€ì´ì˜ í•™ìˆ ë…¼ë¬¸ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
"""

import streamlit as st
import streamlit.components.v1 as components
import sqlite3
import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta
import plotly.express as px
import plotly.graph_objects as go
import yaml
import json
import networkx as nx
from pyvis.network import Network
import tempfile
import os
import re

# í† í”½ í´ëŸ¬ìŠ¤í„°ë§
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# ì§€ë„ ì‹œê°í™”
try:
    import folium
    from streamlit_folium import folium_static
    FOLIUM_AVAILABLE = True
except ImportError:
    FOLIUM_AVAILABLE = False

# Claude API
try:
    from anthropic import Anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Journal Monitor",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS ìŠ¤íƒ€ì¼
st.markdown("""
<style>
    .priority-high { color: #ff4b4b; font-weight: bold; }
    .priority-medium { color: #ffa500; font-weight: bold; }
    .priority-low { color: #808080; }
    .metric-card {
        background-color: #f0f2f6;
        padding: 20px;
        border-radius: 10px;
        text-align: center;
    }
    .article-card {
        padding: 15px;
        border-left: 4px solid #ccc;
        margin-bottom: 10px;
        background-color: #fafafa;
        border-radius: 0 8px 8px 0;
    }
    .article-card.high { border-left-color: #ff4b4b; }
    .article-card.medium { border-left-color: #ffa500; }
    .article-card.low { border-left-color: #808080; }
    
    /* í‚¤ì›Œë“œ íƒœê·¸ ìŠ¤íƒ€ì¼ */
    .keyword-tag {
        display: inline-block;
        padding: 4px 12px;
        margin: 3px;
        border-radius: 15px;
        font-size: 14px;
        font-weight: 500;
    }
    .keyword-tag.high {
        background: linear-gradient(135deg, #ff6b6b, #ee5a5a);
        color: white;
    }
    .keyword-tag.medium {
        background: linear-gradient(135deg, #ffc048, #ffb020);
        color: #333;
    }
    .keyword-tag.count-1 { font-size: 12px; opacity: 0.7; }
    .keyword-tag.count-2 { font-size: 13px; opacity: 0.8; }
    .keyword-tag.count-3 { font-size: 14px; opacity: 0.9; }
    .keyword-tag.count-4 { font-size: 15px; }
    .keyword-tag.count-5 { font-size: 16px; font-weight: 600; }
    
    /* í‚¤ì›Œë“œ ì»¨í…Œì´ë„ˆ */
    .keyword-container {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 20px;
        border-radius: 15px;
        margin-bottom: 20px;
    }
    .keyword-container h4 {
        color: white;
        margin-bottom: 15px;
    }
    .keyword-badges {
        display: flex;
        flex-wrap: wrap;
        gap: 8px;
    }
    .keyword-badge {
        background: rgba(255,255,255,0.9);
        color: #333;
        padding: 6px 14px;
        border-radius: 20px;
        font-size: 13px;
        font-weight: 500;
        display: inline-flex;
        align-items: center;
        gap: 6px;
    }
    .keyword-badge .count {
        background: #667eea;
        color: white;
        padding: 2px 8px;
        border-radius: 10px;
        font-size: 11px;
        font-weight: 600;
    }
</style>
""", unsafe_allow_html=True)


class DashboardDB:
    """ëŒ€ì‹œë³´ë“œìš© ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
    
    def get_connection(self):
        return sqlite3.connect(self.db_path)
    
    def get_stats(self) -> dict:
        """ì „ì²´ í†µê³„"""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            # ì „ì²´ ë…¼ë¬¸ ìˆ˜
            cursor.execute("SELECT COUNT(*) FROM articles")
            total = cursor.fetchone()[0]
            
            # ìš°ì„ ìˆœìœ„ë³„
            cursor.execute("SELECT COUNT(*) FROM articles WHERE priority = 'high'")
            high = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM articles WHERE priority = 'medium'")
            medium = cursor.fetchone()[0]
            
            # ì˜¤ëŠ˜ ìˆ˜ì§‘
            today = datetime.now().strftime('%Y-%m-%d')
            cursor.execute("SELECT COUNT(*) FROM articles WHERE DATE(fetched_at) = ?", (today,))
            today_count = cursor.fetchone()[0]
            
            # ì´ë²ˆ ì£¼
            week_ago = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
            cursor.execute("SELECT COUNT(*) FROM articles WHERE DATE(fetched_at) >= ?", (week_ago,))
            week_count = cursor.fetchone()[0]
            
            # ì´ˆë¡ ìˆëŠ” ë…¼ë¬¸
            cursor.execute("SELECT COUNT(*) FROM articles WHERE abstract IS NOT NULL AND LENGTH(abstract) > 50")
            with_abstract = cursor.fetchone()[0]
            
            return {
                'total': total,
                'high': high,
                'medium': medium,
                'low': total - high - medium,
                'today': today_count,
                'week': week_count,
                'with_abstract': with_abstract
            }
    
    def get_articles(self, priority: str = None, journal: str = None, 
                     days: int = None, search: str = None, 
                     starred_only: bool = False, unread_only: bool = False,
                     limit: int = 100) -> pd.DataFrame:
        """ë…¼ë¬¸ ëª©ë¡ ì¡°íšŒ"""
        query = """
            SELECT 
                a.id,
                a.title,
                a.title_ko,
                a.abstract,
                a.abstract_ko,
                a.summary_ko,
                a.url,
                a.doi,
                a.priority,
                a.keywords_matched,
                a.fetched_at,
                a.published_date,
                a.is_read,
                a.is_starred,
                j.name as journal_name,
                j.category
            FROM articles a
            LEFT JOIN journals j ON a.journal_id = j.id
            WHERE 1=1
        """
        params = []
        
        if priority and priority != "ì „ì²´":
            query += " AND a.priority = ?"
            params.append(priority.lower())
        
        if journal and journal != "ì „ì²´":
            query += " AND j.name = ?"
            params.append(journal)
        
        if days:
            date_from = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
            query += " AND DATE(a.fetched_at) >= ?"
            params.append(date_from)
        
        if search:
            query += " AND (a.title LIKE ? OR a.abstract LIKE ? OR a.title_ko LIKE ? OR a.keywords_matched LIKE ?)"
            search_term = f"%{search}%"
            params.extend([search_term, search_term, search_term, search_term])
        
        if starred_only:
            query += " AND a.is_starred = 1"
        
        if unread_only:
            query += " AND a.is_read = 0"
        
        query += " ORDER BY a.fetched_at DESC LIMIT ?"
        params.append(limit)
        
        with self.get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=params)
        
        return df
    
    def get_journals(self) -> list:
        """ì €ë„ ëª©ë¡"""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT DISTINCT name FROM journals ORDER BY name")
            return [row[0] for row in cursor.fetchall()]
    
    def get_daily_counts(self, days: int = 30) -> pd.DataFrame:
        """ì¼ë³„ ìˆ˜ì§‘ í˜„í™©"""
        date_from = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
        
        query = """
            SELECT 
                DATE(fetched_at) as date,
                COUNT(*) as count,
                SUM(CASE WHEN priority = 'high' THEN 1 ELSE 0 END) as high,
                SUM(CASE WHEN priority = 'medium' THEN 1 ELSE 0 END) as medium
            FROM articles
            WHERE DATE(fetched_at) >= ?
            GROUP BY DATE(fetched_at)
            ORDER BY date
        """
        
        with self.get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=[date_from])
        
        return df
    
    def get_journal_distribution(self) -> pd.DataFrame:
        """ì €ë„ë³„ ë¶„í¬"""
        query = """
            SELECT 
                j.name as journal,
                COUNT(*) as count,
                SUM(CASE WHEN a.priority = 'high' THEN 1 ELSE 0 END) as high,
                SUM(CASE WHEN a.priority = 'medium' THEN 1 ELSE 0 END) as medium
            FROM articles a
            LEFT JOIN journals j ON a.journal_id = j.id
            GROUP BY j.name
            ORDER BY count DESC
            LIMIT 20
        """
        
        with self.get_connection() as conn:
            df = pd.read_sql_query(query, conn)
        
        return df
    
    def get_keyword_stats(self, days: int = None) -> pd.DataFrame:
        """í‚¤ì›Œë“œ í†µê³„ (ê¸°ê°„ í•„í„° ì˜µì…˜)"""
        if days:
            date_from = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
            query = """
                SELECT keywords_matched, priority
                FROM articles
                WHERE keywords_matched IS NOT NULL 
                  AND keywords_matched != ''
                  AND DATE(fetched_at) >= ?
            """
            params = [date_from]
        else:
            query = """
                SELECT keywords_matched, priority
                FROM articles
                WHERE keywords_matched IS NOT NULL AND keywords_matched != ''
            """
            params = []
        
        with self.get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=params)
        
        # í‚¤ì›Œë“œ íŒŒì‹± ë° ì§‘ê³„
        keyword_counts = {}
        keyword_priorities = {}  # í‚¤ì›Œë“œë³„ ìµœê³  ìš°ì„ ìˆœìœ„ ì¶”ì 
        
        for _, row in df.iterrows():
            keywords = row['keywords_matched']
            priority = row['priority']
            if keywords:
                try:
                    kw_list = json.loads(keywords)
                    if isinstance(kw_list, list):
                        for kw in kw_list:
                            kw = str(kw).strip()
                            if kw:
                                keyword_counts[kw] = keyword_counts.get(kw, 0) + 1
                                # highê°€ mediumë³´ë‹¤ ìš°ì„ 
                                if kw not in keyword_priorities or priority == 'high':
                                    keyword_priorities[kw] = priority
                except (json.JSONDecodeError, TypeError):
                    for kw in keywords.split(','):
                        kw = kw.strip()
                        if kw:
                            keyword_counts[kw] = keyword_counts.get(kw, 0) + 1
                            if kw not in keyword_priorities or priority == 'high':
                                keyword_priorities[kw] = priority
        
        result = pd.DataFrame([
            {'keyword': k, 'count': v, 'priority': keyword_priorities.get(k, 'normal')} 
            for k, v in sorted(keyword_counts.items(), key=lambda x: -x[1])
        ])
        
        return result
    
    def get_today_keywords(self) -> pd.DataFrame:
        """ì˜¤ëŠ˜ ìˆ˜ì§‘ëœ ë…¼ë¬¸ì˜ í‚¤ì›Œë“œ í†µê³„"""
        return self.get_keyword_stats(days=1)
    
    def mark_as_read(self, article_id: int):
        """ì½ìŒ í‘œì‹œ"""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(
                "UPDATE articles SET is_read = 1 WHERE id = ?",
                (article_id,)
            )
            conn.commit()
    
    def toggle_starred(self, article_id: int):
        """ì¦ê²¨ì°¾ê¸° í† ê¸€"""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(
                "UPDATE articles SET is_starred = CASE WHEN is_starred = 1 THEN 0 ELSE 1 END WHERE id = ?",
                (article_id,)
            )
            conn.commit()
    
    def toggle_read(self, article_id: int):
        """ì½ìŒ í‘œì‹œ í† ê¸€"""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(
                "UPDATE articles SET is_read = CASE WHEN is_read = 1 THEN 0 ELSE 1 END WHERE id = ?",
                (article_id,)
            )
            conn.commit()


def load_config() -> dict:
    """config.yaml ë¡œë“œ"""
    config_path = Path("./config.yaml")
    if config_path.exists():
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    return {}


def save_config(config: dict):
    """config.yaml ì €ì¥"""
    config_path = Path("./config.yaml")
    with open(config_path, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, allow_unicode=True, default_flow_style=False, sort_keys=False)


def render_article_card(article: pd.Series, db: 'DashboardDB' = None):
    """ë…¼ë¬¸ ì¹´ë“œ ë Œë”ë§"""
    priority = article.get('priority', 'normal') or 'normal'
    priority_emoji = {'high': 'ğŸ”´', 'medium': 'ğŸŸ¡', 'normal': 'âšª', 'low': 'âšª'}.get(priority, 'âšª')
    
    title_en = article.get('title', 'No Title')
    title_ko = article.get('title_ko', '')
    
    if title_ko:
        display_title = f"{title_ko} ({title_en})"
    else:
        display_title = title_en
    
    journal = article.get('journal_name', 'Unknown')
    fetched = article.get('fetched_at', '')[:10] if article.get('fetched_at') else ''
    
    keywords = article.get('keywords_matched', '')
    if keywords:
        try:
            kw_list = json.loads(keywords)
            if isinstance(kw_list, list):
                keywords = ', '.join(kw_list)
        except:
            pass
    
    # ìƒíƒœ í™•ì¸
    article_id = article.get('id')
    is_starred = bool(article.get('is_starred', 0))
    is_read = bool(article.get('is_read', 0))
    
    with st.container():
        col1, col2, col3, col4 = st.columns([0.85, 0.05, 0.05, 0.05])
        
        with col1:
            # ì œëª©ì— ì½ìŒ í‘œì‹œ ë°˜ì˜
            read_style = "" if not is_read else "~~"
            star_mark = "â­ " if is_starred else ""
            st.markdown(f"### {star_mark}{priority_emoji} {display_title}")
            st.caption(f"ğŸ“° {journal} Â· ğŸ“… {fetched}{' Â· âœ… ì½ìŒ' if is_read else ''}")
            
            if keywords:
                st.markdown(f"ğŸ·ï¸ `{keywords}`")
        
        with col2:
            # ì¦ê²¨ì°¾ê¸° ë²„íŠ¼
            star_icon = "â­" if is_starred else "â˜†"
            if st.button(star_icon, key=f"star_{article_id}", help="ì¦ê²¨ì°¾ê¸° í† ê¸€"):
                if db:
                    db.toggle_starred(article_id)
                    st.rerun()
        
        with col3:
            # ì½ìŒ í‘œì‹œ ë²„íŠ¼
            read_icon = "âœ…" if is_read else "â˜"
            if st.button(read_icon, key=f"read_{article_id}", help="ì½ìŒ í‘œì‹œ í† ê¸€"):
                if db:
                    db.toggle_read(article_id)
                    st.rerun()
        
        with col4:
            if article.get('url'):
                st.link_button("ğŸ”—", article['url'], help="ì›ë¬¸ ë³´ê¸°")
        
        abstract_en = article.get('abstract', '')
        abstract_ko = article.get('abstract_ko', '')
        summary_ko = article.get('summary_ko', '')
        
        if abstract_en or abstract_ko or summary_ko:
            with st.expander("ğŸ“„ ìƒì„¸ ë³´ê¸°"):
                if summary_ko:
                    st.markdown("**ğŸ’¡ ìš”ì•½:**")
                    st.markdown(summary_ko)
                    st.divider()
                
                if abstract_en:
                    st.markdown("**ğŸ“ Abstract (ì˜ë¬¸):**")
                    st.markdown(abstract_en)
                
                if abstract_ko:
                    st.markdown("")
                    st.markdown("**ğŸ“ ì´ˆë¡ (í•œêµ­ì–´ ë²ˆì—­):**")
                    st.markdown(abstract_ko)
                
                if article.get('doi'):
                    st.divider()
                    doi = article['doi']
                    doi_url = f"https://doi.org/{doi}" if not doi.startswith('http') else doi
                    st.markdown(f"**DOI:** [{doi}]({doi_url})")
        
        st.divider()


def render_today_keywords(db: DashboardDB):
    """ì˜¤ëŠ˜ì˜ í‚¤ì›Œë“œ ì¸í¬ê·¸ë˜í”½ - Streamlit ë„¤ì´í‹°ë¸Œ ë²„ì „"""
    today_kw = db.get_today_keywords()
    
    if today_kw.empty:
        st.info("ì˜¤ëŠ˜ ìˆ˜ì§‘ëœ ë…¼ë¬¸ì—ì„œ ë§¤ì¹­ëœ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ìƒìœ„ 12ê°œ í‚¤ì›Œë“œ
    top_keywords = today_kw.head(12)
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.markdown("#### ğŸ·ï¸ í‚¤ì›Œë“œ íƒœê·¸")
        st.caption("í´ë¦­í•˜ë©´ í•´ë‹¹ í‚¤ì›Œë“œ ë…¼ë¬¸ ëª©ë¡ìœ¼ë¡œ ì´ë™")
        
        # 4ì—´ë¡œ í‚¤ì›Œë“œ ë²„íŠ¼ ë°°ì¹˜
        kw_cols = st.columns(4)
        
        for i, (_, row) in enumerate(top_keywords.iterrows()):
            kw = row['keyword']
            count = row['count']
            priority = row.get('priority', 'normal')
            
            # ìš°ì„ ìˆœìœ„ë³„ ì´ëª¨ì§€
            if priority == 'high':
                emoji = "ğŸ”´"
            elif priority == 'medium':
                emoji = "ğŸŸ¡"
            else:
                emoji = "ğŸ”µ"
            
            with kw_cols[i % 4]:
                # ë²„íŠ¼ í´ë¦­ ì‹œ í•´ë‹¹ í‚¤ì›Œë“œë¡œ ë…¼ë¬¸ ëª©ë¡ í˜ì´ì§€ ì´ë™
                if st.button(f"{emoji} {kw} ({count})", key=f"kw_btn_{i}", use_container_width=True):
                    st.session_state.selected_keyword = kw
                    st.session_state.selected_menu = "ğŸ“‘ ë…¼ë¬¸ ëª©ë¡"
                    st.rerun()
        
        st.caption("ğŸ”´ High Â· ğŸŸ¡ Medium Â· ğŸ”µ ê¸°íƒ€")
    
    with col2:
        st.markdown("#### ğŸ“Š í‚¤ì›Œë“œ ë¹ˆë„")
        
        if not top_keywords.empty:
            # ìƒ‰ìƒ ë§¤í•‘
            colors = []
            for _, row in top_keywords.iterrows():
                if row.get('priority') == 'high':
                    colors.append('#ff4b4b')
                elif row.get('priority') == 'medium':
                    colors.append('#ffa500')
                else:
                    colors.append('#4A90D9')
            
            fig = go.Figure(go.Bar(
                x=top_keywords['count'].values,
                y=top_keywords['keyword'].values,
                orientation='h',
                marker_color=colors,
                text=top_keywords['count'].values,
                textposition='auto',
            ))
            
            fig.update_layout(
                yaxis={'categoryorder': 'total ascending'},
                height=350,
                margin=dict(l=0, r=0, t=10, b=0),
                xaxis_title="",
                yaxis_title="",
                showlegend=False
            )
            
            st.plotly_chart(fig, use_container_width=True)


def main():
    """ë©”ì¸ ëŒ€ì‹œë³´ë“œ"""
    
    db_path = Path("./data/journals.db")
    
    if not db_path.exists():
        st.error(f"ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {db_path}")
        st.info("JournalMonitorë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return
    
    db = DashboardDB(str(db_path))
    
    # session_state ì´ˆê¸°í™”
    if 'selected_keyword' not in st.session_state:
        st.session_state.selected_keyword = None
    if 'selected_menu' not in st.session_state:
        st.session_state.selected_menu = None
    
    with st.sidebar:
        st.title("ğŸ“š Journal Monitor")
        st.caption("ì¼€ì´ì˜ í•™ìˆ ë…¼ë¬¸ ëª¨ë‹ˆí„°ë§")
        
        st.divider()
        
        # í‚¤ì›Œë“œ í´ë¦­ìœ¼ë¡œ ë©”ë‰´ ì´ë™ ì‹œ ë°˜ì˜
        default_index = 0
        menu_options = ["ğŸ  í™ˆ", "ğŸ“‘ ë…¼ë¬¸ ëª©ë¡", "ğŸ“Š ê¸°ê°„ ë¶„ì„", "ğŸ“ˆ í†µê³„", "âš™ï¸ ì„¤ì •"]
        if st.session_state.selected_menu:
            if st.session_state.selected_menu in menu_options:
                default_index = menu_options.index(st.session_state.selected_menu)
        
        menu = st.radio(
            "ë©”ë‰´",
            menu_options,
            index=default_index,
            label_visibility="collapsed"
        )
        
        st.divider()
        
        stats = db.get_stats()
        st.metric("ì´ ë…¼ë¬¸", f"{stats['total']:,}í¸")
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("ğŸ”´ High", stats['high'])
        with col2:
            st.metric("ğŸŸ¡ Medium", stats['medium'])
        
        st.metric("ì˜¤ëŠ˜ ìˆ˜ì§‘", f"{stats['today']}í¸")
    
    if menu == "ğŸ  í™ˆ":
        render_home(db, stats)
    elif menu == "ğŸ“‘ ë…¼ë¬¸ ëª©ë¡":
        render_articles(db)
    elif menu == "ğŸ“Š ê¸°ê°„ ë¶„ì„":
        render_period_analysis(db)
    elif menu == "ğŸ“ˆ í†µê³„":
        render_statistics(db)
    elif menu == "âš™ï¸ ì„¤ì •":
        render_settings()


def render_home(db: DashboardDB, stats: dict):
    """í™ˆ í™”ë©´"""
    st.title("ğŸ“š í•™ìˆ ë…¼ë¬¸ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ")
    
    # ìš”ì•½ ì¹´ë“œ
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ì˜¤ëŠ˜ ìˆ˜ì§‘", f"{stats['today']}í¸")
    
    with col2:
        st.metric("ì´ë²ˆ ì£¼", f"{stats['week']}í¸")
    
    with col3:
        st.metric("ğŸ”´ High Priority", f"{stats['high']}í¸")
    
    with col4:
        st.metric("ì´ˆë¡ ë³´ìœ ìœ¨", f"{stats['with_abstract'] / stats['total'] * 100:.1f}%" if stats['total'] > 0 else "0%")
    
    st.divider()
    
    # ===== ì˜¤ëŠ˜ì˜ í‚¤ì›Œë“œ ì¸í¬ê·¸ë˜í”½ (ìƒˆë¡œ ì¶”ê°€) =====
    st.subheader("ğŸ¯ ì˜¤ëŠ˜ì˜ ì—°êµ¬ í‚¤ì›Œë“œ")
    render_today_keywords(db)
    
    st.divider()
    
    # ìµœê·¼ ìˆ˜ì§‘ íŠ¸ë Œë“œ
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ“Š ìµœê·¼ 30ì¼ ìˆ˜ì§‘ í˜„í™©")
        daily = db.get_daily_counts(30)
        
        if not daily.empty:
            fig = px.bar(
                daily, 
                x='date', 
                y='count',
                color_discrete_sequence=['#4A90D9']
            )
            fig.update_layout(
                xaxis_title="",
                yaxis_title="ë…¼ë¬¸ ìˆ˜",
                showlegend=False,
                height=300
            )
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    with col2:
        st.subheader("ğŸ“° ì €ë„ë³„ ë¶„í¬")
        journal_dist = db.get_journal_distribution()
        
        if not journal_dist.empty:
            fig = px.pie(
                journal_dist.head(10),
                values='count',
                names='journal',
                hole=0.4
            )
            fig.update_layout(height=300)
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    st.divider()
    
    # ìµœê·¼ High Priority ë…¼ë¬¸
    st.subheader("ğŸ”´ ìµœê·¼ High Priority ë…¼ë¬¸")
    
    high_articles = db.get_articles(priority='high', limit=5)
    
    if not high_articles.empty:
        for _, article in high_articles.iterrows():
            render_article_card(article, db=db)
    else:
        st.info("High priority ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")


def render_articles(db: DashboardDB):
    """ë…¼ë¬¸ ëª©ë¡ í™”ë©´"""
    st.title("ğŸ“‘ ë…¼ë¬¸ ëª©ë¡")
    
    # í‚¤ì›Œë“œì—ì„œ ì´ë™í•´ì˜¨ ê²½ìš° ê²€ìƒ‰ì–´ ìë™ ì„¤ì •
    default_search = ""
    if st.session_state.get('selected_keyword'):
        default_search = st.session_state.selected_keyword
        st.info(f"ğŸ·ï¸ '{default_search}' í‚¤ì›Œë“œ ë…¼ë¬¸ ëª©ë¡")
        # ì‚¬ìš© í›„ ì´ˆê¸°í™” (ë‹¤ìŒ ë°©ë¬¸ ì‹œ ë¦¬ì…‹)
        st.session_state.selected_keyword = None
        st.session_state.selected_menu = None
    
    # í•„í„° ì˜µì…˜ - 1í–‰: ê¸°ë³¸ í•„í„°
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        priority_filter = st.selectbox("ìš°ì„ ìˆœìœ„", ["ì „ì²´", "high", "medium", "normal"])
    
    with col2:
        journals = ["ì „ì²´"] + db.get_journals()
        journal_filter = st.selectbox("ì €ë„", journals)
    
    with col3:
        days_options = [("ì „ì²´", None), ("ì˜¤ëŠ˜", 1), ("ìµœê·¼ 7ì¼", 7), ("ìµœê·¼ 30ì¼", 30)]
        days_filter = st.selectbox("ê¸°ê°„", days_options, format_func=lambda x: x[0])
    
    with col4:
        search = st.text_input("ğŸ” ê²€ìƒ‰", value=default_search, placeholder="ì œëª©, ì´ˆë¡, í‚¤ì›Œë“œ...")
    
    # í•„í„° ì˜µì…˜ - 2í–‰: ì¦ê²¨ì°¾ê¸°/ì½ìŒ í•„í„°
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        starred_only = st.toggle("â­ ì¦ê²¨ì°¾ê¸°ë§Œ", value=False)
    with col2:
        unread_only = st.toggle("â˜ ì•ˆì½ì€ ê²ƒë§Œ", value=False)
    
    st.divider()
    
    articles = db.get_articles(
        priority=priority_filter if priority_filter != "ì „ì²´" else None,
        journal=journal_filter if journal_filter != "ì „ì²´" else None,
        days=days_filter[1],
        search=search if search else None,
        starred_only=starred_only,
        unread_only=unread_only,
        limit=50
    )
    
    st.caption(f"ì´ {len(articles)}í¸")
    
    if not articles.empty:
        for _, article in articles.iterrows():
            render_article_card(article, db=db)
    else:
        st.info("ì¡°ê±´ì— ë§ëŠ” ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")


def render_period_analysis(db: DashboardDB):
    """ê¸°ê°„ ë¶„ì„ í˜ì´ì§€"""
    st.title("ğŸ“Š ê¸°ê°„ ë¶„ì„")
    
    st.markdown("""
    ì„ íƒí•œ ê¸°ê°„ ë™ì•ˆì˜ ë…¼ë¬¸ ìˆ˜ì§‘ í˜„í™©, í‚¤ì›Œë“œ íŠ¸ë Œë“œ, ì €ë„ ë¶„í¬ ë“±ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    """)
    
    # ========== ê¸°ê°„ ì„ íƒ UI ==========
    st.subheader("ğŸ“… ë¶„ì„ ê¸°ê°„ ì„ íƒ")
    
    col1, col2, col3 = st.columns([2, 1, 1])
    
    with col1:
        period_options = {
            "1ê°œì›”": 30,
            "3ê°œì›”": 90,
            "6ê°œì›”": 180,
            "12ê°œì›”": 365,
            "ì»¤ìŠ¤í…€": None
        }
        selected_period = st.radio(
            "ê¸°ê°„ ì„ íƒ",
            list(period_options.keys()),
            horizontal=True,
            label_visibility="collapsed"
        )
    
    # ì»¤ìŠ¤í…€ ë‚ ì§œ ì„ íƒ
    if selected_period == "ì»¤ìŠ¤í…€":
        col1, col2 = st.columns(2)
        with col1:
            start_date = st.date_input("ì‹œì‘ì¼", value=datetime.now() - timedelta(days=30))
        with col2:
            end_date = st.date_input("ì¢…ë£Œì¼", value=datetime.now())
        days = (end_date - start_date).days
        date_from = start_date.strftime('%Y-%m-%d')
        date_to = end_date.strftime('%Y-%m-%d')
    else:
        days = period_options[selected_period]
        date_from = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
        date_to = datetime.now().strftime('%Y-%m-%d')
    
    st.caption(f"ğŸ“† ë¶„ì„ ê¸°ê°„: **{date_from}** ~ **{date_to}** ({days}ì¼)")
    
    st.divider()
    
    # ========== ë°ì´í„° ì¡°íšŒ ==========
    period_stats = get_period_stats(db, days)
    period_keywords = db.get_keyword_stats(days=days)
    period_daily = db.get_daily_counts(days=days)
    
    # ========== 1. í•µì‹¬ ìš”ì•½ ==========
    st.subheader("ğŸ“ í•µì‹¬ ìš”ì•½")
    
    col1, col2, col3, col4, col5 = st.columns(5)
    
    with col1:
        st.metric("ì´ ë…¼ë¬¸ ìˆ˜", f"{period_stats['total']:,}í¸")
    with col2:
        st.metric("ğŸ”´ High", f"{period_stats['high']}í¸")
    with col3:
        st.metric("ğŸŸ¡ Medium", f"{period_stats['medium']}í¸")
    with col4:
        avg_daily = period_stats['total'] / days if days > 0 else 0
        st.metric("ì¼í‰ê· ", f"{avg_daily:.1f}í¸")
    with col5:
        read_rate = (period_stats['read'] / period_stats['total'] * 100) if period_stats['total'] > 0 else 0
        st.metric("ì½ìŒë¥ ", f"{read_rate:.1f}%")
    
    # Top 5 í‚¤ì›Œë“œ í‘œì‹œ
    if not period_keywords.empty:
        top5 = period_keywords.head(5)
        top5_str = " Â· ".join([f"**{row['keyword']}**({row['count']})".replace('**', '') for _, row in top5.iterrows()])
        st.info(f"ğŸ·ï¸ ì£¼ìš” í‚¤ì›Œë“œ: {top5_str}")
    
    st.divider()
    
    # ========== 2. í‚¤ì›Œë“œ íŠ¸ë Œë“œ ==========
    st.subheader("ğŸ·ï¸ í‚¤ì›Œë“œ ë¶„ì„")
    
    tab1, tab2, tab3 = st.tabs(["í‚¤ì›Œë“œ ë¹ˆë„", "í‚¤ì›Œë“œ íŠ¸ë Œë“œ", "ğŸ”— ê³µì¶œí˜„ ë„¤íŠ¸ì›Œí¬"])
    
    with tab1:
        if not period_keywords.empty:
            col1, col2 = st.columns([1, 1])
            
            with col1:
                # ê°€ë¡œ ë§‰ëŒ€ ì°¨íŠ¸
                top20 = period_keywords.head(20)
                
                colors = []
                for _, row in top20.iterrows():
                    if row.get('priority') == 'high':
                        colors.append('#ff4b4b')
                    elif row.get('priority') == 'medium':
                        colors.append('#ffa500')
                    else:
                        colors.append('#4A90D9')
                
                fig = go.Figure(go.Bar(
                    x=top20['count'].values,
                    y=top20['keyword'].values,
                    orientation='h',
                    marker_color=colors,
                    text=top20['count'].values,
                    textposition='auto',
                ))
                
                fig.update_layout(
                    title="í‚¤ì›Œë“œ ë¹ˆë„ Top 20",
                    yaxis={'categoryorder': 'total ascending'},
                    height=500,
                    margin=dict(l=0, r=0, t=40, b=0),
                    xaxis_title="",
                    yaxis_title="",
                )
                
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # íŒŒì´ ì°¨íŠ¸
                top10 = period_keywords.head(10)
                
                fig = px.pie(
                    top10,
                    values='count',
                    names='keyword',
                    title="í‚¤ì›Œë“œ ë¹„ìœ¨ Top 10",
                    hole=0.4
                )
                fig.update_layout(height=500)
                st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("í•´ë‹¹ ê¸°ê°„ì— í‚¤ì›Œë“œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    with tab3:
        st.markdown("""
        ê°™ì€ ë…¼ë¬¸ì— í•¨ê»˜ ë“±ì¥í•œ í‚¤ì›Œë“œë“¤ì„ ë„¤íŠ¸ì›Œí¬ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.  
        - ë…¸ë“œ í¬ê¸° = ì—°ê²° ìˆ˜ (ë‹¤ë¥¸ í‚¤ì›Œë“œì™€ ì–¼ë§ˆë‚˜ ìì£¼ í•¨ê»˜ ë“±ì¥í•˜ëŠ”ì§€)
        - ì—£ì§€ ë‘ê»˜ = ê³µì¶œí˜„ íšŸìˆ˜
        - ğŸ”´ High Priority Â· ğŸŸ¡ Medium Priority Â· ğŸ”µ ê¸°íƒ€
        """)
        
        col1, col2 = st.columns([3, 1])
        with col2:
            min_cooccur = st.slider("ìµœì†Œ ê³µì¶œí˜„ íšŸìˆ˜", 1, 10, 2, help="ì´ íšŸìˆ˜ ì´ìƒ í•¨ê»˜ ë“±ì¥í•œ í‚¤ì›Œë“œë§Œ í‘œì‹œ")
        
        cooccurrence_df = get_keyword_cooccurrence(db, days, min_count=min_cooccur)
        
        if not cooccurrence_df.empty:
            st.caption(f"í‚¤ì›Œë“œ ì—°ê²° ìˆ˜: {len(cooccurrence_df)}ê°œ")
            render_keyword_network(cooccurrence_df, period_keywords)
            
            # ê³µì¶œí˜„ Top 10 í…Œì´ë¸”
            with st.expander("ğŸ“Š ê³µì¶œí˜„ Top 10 ë³´ê¸°"):
                top10 = cooccurrence_df.head(10)
                for i, row in top10.iterrows():
                    st.markdown(f"**{row['source']}** â†” **{row['target']}**: {row['weight']}íšŒ")
        else:
            st.info("ê³µì¶œí˜„ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ê¸°ê°„ì„ ëŠ˜ë¦¬ê±°ë‚˜ ìµœì†Œ ê³µì¶œí˜„ íšŸìˆ˜ë¥¼ ë‚®ì¶°ë³´ì„¸ìš”.")
    
    with tab2:
        # ì¼ë³„ í‚¤ì›Œë“œ íŠ¸ë Œë“œ (ìƒìœ„ 5ê°œ í‚¤ì›Œë“œ)
        if not period_keywords.empty:
            st.markdown("ì£¼ìš” í‚¤ì›Œë“œì˜ ì¼ë³„ ë“±ì¥ íŠ¸ë Œë“œ")
            
            top5_keywords = period_keywords.head(5)['keyword'].tolist()
            keyword_trend = get_keyword_daily_trend(db, days, top5_keywords)
            
            if not keyword_trend.empty:
                fig = px.line(
                    keyword_trend,
                    x='date',
                    y='count',
                    color='keyword',
                    title="ì£¼ìš” í‚¤ì›Œë“œ ì¼ë³„ íŠ¸ë Œë“œ",
                    markers=True
                )
                fig.update_layout(height=400)
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("íŠ¸ë Œë“œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        else:
            st.info("í•´ë‹¹ ê¸°ê°„ì— í‚¤ì›Œë“œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    st.divider()
    
    # ========== 3. ìˆ˜ì§‘ í˜„í™© ==========
    st.subheader("ğŸ“ˆ ìˆ˜ì§‘ í˜„í™©")
    
    if not period_daily.empty:
        fig = go.Figure()
        
        fig.add_trace(go.Scatter(
            x=period_daily['date'],
            y=period_daily['count'],
            mode='lines+markers',
            name='ì „ì²´',
            line=dict(color='#4A90D9', width=2)
        ))
        
        fig.add_trace(go.Bar(
            x=period_daily['date'],
            y=period_daily['high'],
            name='High',
            marker_color='#ff4b4b'
        ))
        
        fig.add_trace(go.Bar(
            x=period_daily['date'],
            y=period_daily['medium'],
            name='Medium',
            marker_color='#ffa500'
        ))
        
        fig.update_layout(
            title="ì¼ë³„ ë…¼ë¬¸ ìˆ˜ì§‘ í˜„í™©",
            xaxis_title="",
            yaxis_title="ë…¼ë¬¸ ìˆ˜",
            barmode='stack',
            height=400
        )
        
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("í•´ë‹¹ ê¸°ê°„ì— ìˆ˜ì§‘ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    st.divider()
    
    # ========== 4. ì €ë„ë³„ ë¶„í¬ ==========
    st.subheader("ğŸ“° ì €ë„ë³„ ë¶„í¬")
    
    journal_stats = get_period_journal_stats(db, days)
    
    if not journal_stats.empty:
        col1, col2 = st.columns([1, 1])
        
        with col1:
            fig = px.bar(
                journal_stats,
                x='count',
                y='journal',
                orientation='h',
                color='high',
                color_continuous_scale='Reds',
                title="ì €ë„ë³„ ë…¼ë¬¸ ìˆ˜"
            )
            fig.update_layout(
                yaxis={'categoryorder': 'total ascending'},
                height=500,
                xaxis_title="ë…¼ë¬¸ ìˆ˜",
                yaxis_title=""
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            fig = px.pie(
                journal_stats.head(10),
                values='count',
                names='journal',
                title="ì €ë„ ë¹„ìœ¨ Top 10",
                hole=0.4
            )
            fig.update_layout(height=500)
            st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("í•´ë‹¹ ê¸°ê°„ì— ì €ë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    st.divider()
    
    # ========== 5. ê°œì¸ í™œë™ ìš”ì•½ ==========
    st.subheader("ğŸ“š ê°œì¸ í™œë™ ìš”ì•½")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("â­ ì¦ê²¨ì°¾ê¸°", f"{period_stats['starred']}í¸")
    with col2:
        st.metric("âœ… ì½ìŒ", f"{period_stats['read']}í¸")
    with col3:
        unread = period_stats['total'] - period_stats['read']
        st.metric("â˜ ì•ˆì½ìŒ", f"{unread}í¸")
    
    # ì¦ê²¨ì°¾ê¸° ë…¼ë¬¸ ëª©ë¡
    if period_stats['starred'] > 0:
        with st.expander(f"â­ ì¦ê²¨ì°¾ê¸°í•œ ë…¼ë¬¸ ({period_stats['starred']}í¸)"):
            starred_articles = db.get_articles(days=days, starred_only=True, limit=20)
            for _, article in starred_articles.iterrows():
                priority_emoji = {'high': 'ğŸ”´', 'medium': 'ğŸŸ¡'}.get(article.get('priority'), 'âšª')
                title = article.get('title_ko') or article.get('title')
                st.markdown(f"- {priority_emoji} **{title}**")
    
    st.divider()
    
    # ========== 6. ì´ë¡  ì—°ê²°ë§ ==========
    st.subheader("ğŸ§  ì´ë¡  ì—°ê²°ë§")
    
    st.markdown("""
    ë…¼ë¬¸ì—ì„œ ì–¸ê¸‰ëœ ì´ë¡ ê°€ì™€ ì´ë¡ ì  ê°œë…ë“¤ì˜ ì—°ê²° íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    - ğŸŸ£ **ë³´ë¼**: ì´ë¡ ê°€ (Foucault, Deleuze, Lefebvre ë“±)
    - ğŸŸ¦ **íŒŒë‘**: ì´ë¡ ì  ê°œë… (governmentality, assemblage ë“±)
    """)
    
    theory_data = analyze_theory_connections(db, days)
    
    if theory_data['theorists'] or theory_data['concepts']:
        col1, col2 = st.columns(2)
        
        with col1:
            if theory_data['theorists']:
                st.markdown("**ğŸ“š ì£¼ìš” ì´ë¡ ê°€ ì–¸ê¸‰ íšŸìˆ˜**")
                for name, count in sorted(theory_data['theorists'].items(), key=lambda x: -x[1])[:10]:
                    st.markdown(f"- {name}: **{count}**íšŒ")
        
        with col2:
            if theory_data['concepts']:
                st.markdown("**ğŸ’¡ ì£¼ìš” ê°œë… ì–¸ê¸‰ íšŸìˆ˜**")
                for name, count in sorted(theory_data['concepts'].items(), key=lambda x: -x[1])[:10]:
                    st.markdown(f"- {name}: **{count}**íšŒ")
        
        st.markdown("---")
        st.markdown("**ì´ë¡ ê°€-ê°œë… ì—°ê²°ë§**")
        render_theory_network(theory_data)
    else:
        st.info("í•´ë‹¹ ê¸°ê°„ì— ì´ë¡ ì  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ê¸°ê°„ì„ ëŠ˜ë ¤ë³´ì„¸ìš”.")
    
    st.divider()
    
    # ========== 7. í† í”½ í´ëŸ¬ìŠ¤í„°ë§ ==========
    st.subheader("ğŸ“š í† í”½ í´ëŸ¬ìŠ¤í„°ë§")
    
    st.markdown("""
    ë…¼ë¬¸ ì´ˆë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ìë™ í´ëŸ¬ìŠ¤í„°ë§í•˜ì—¬ ì—°êµ¬ ì£¼ì œë¥¼ ë°œê²¬í•©ë‹ˆë‹¤.  
    TF-IDF + KMeans ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©.
    """)
    
    col1, col2 = st.columns([3, 1])
    with col2:
        n_clusters = st.slider("í´ëŸ¬ìŠ¤í„° ìˆ˜", 3, 10, 5)
    
    clustering_result = perform_topic_clustering(db, days, n_clusters=n_clusters)
    
    if clustering_result['error']:
        st.warning(f"í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {clustering_result['error']}. ë°ì´í„°ê°€ ë” í•„ìš”í•©ë‹ˆë‹¤.")
    else:
        # í´ëŸ¬ìŠ¤í„° ì‹œê°í™” (Scatter plot)
        df_cluster = clustering_result['articles']
        
        fig = px.scatter(
            df_cluster,
            x='x',
            y='y',
            color='cluster',
            hover_data=['title_ko', 'priority'],
            title="ë…¼ë¬¸ í´ëŸ¬ìŠ¤í„° ë¶„í¬ (PCA 2D)",
            color_continuous_scale='viridis'
        )
        fig.update_layout(height=500)
        st.plotly_chart(fig, use_container_width=True)
        
        # í´ëŸ¬ìŠ¤í„°ë³„ ëŒ€í‘œ í‚¤ì›Œë“œ
        st.markdown("**í´ëŸ¬ìŠ¤í„°ë³„ ëŒ€í‘œ í‚¤ì›Œë“œ**")
        
        cols = st.columns(min(n_clusters, 5))
        for i, keywords in clustering_result['clusters'].items():
            with cols[i % 5]:
                cluster_count = len(df_cluster[df_cluster['cluster'] == i])
                st.markdown(f"**í´ëŸ¬ìŠ¤í„° {i+1}** ({cluster_count}í¸)")
                st.caption(", ".join(keywords))
        
        # í´ëŸ¬ìŠ¤í„°ë³„ ë…¼ë¬¸ ëª©ë¡
        with st.expander("í´ëŸ¬ìŠ¤í„°ë³„ ë…¼ë¬¸ ëª©ë¡ ë³´ê¸°"):
            for i in range(n_clusters):
                cluster_articles = df_cluster[df_cluster['cluster'] == i]
                st.markdown(f"### í´ëŸ¬ìŠ¤í„° {i+1}: {', '.join(clustering_result['clusters'][i][:3])}")
                for _, art in cluster_articles.head(5).iterrows():
                    title = art.get('title_ko') or art.get('title')
                    priority_emoji = {'high': 'ğŸ”´', 'medium': 'ğŸŸ¡'}.get(art.get('priority'), 'âšª')
                    st.markdown(f"- {priority_emoji} {title}")
                st.markdown("---")
    
    st.divider()
    
    # ========== 8. ì‚¬ë¡€ ì§€ì—­ ì§€ë„ ==========
    st.subheader("ğŸ—ºï¸ ì‚¬ë¡€ ì§€ì—­ ë¶„í¬")
    
    st.markdown("""
    ë…¼ë¬¸ì—ì„œ ì–¸ê¸‰ëœ ë„ì‹œ/ì§€ì—­ì„ ì§€ë„ì— í‘œì‹œí•©ë‹ˆë‹¤.  
    ì› í¬ê¸°ëŠ” ì–¸ê¸‰ íšŸìˆ˜ì— ë¹„ë¡€í•©ë‹ˆë‹¤.
    """)
    
    location_df = extract_locations(db, days)
    
    if not location_df.empty:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            render_location_map(location_df)
        
        with col2:
            st.markdown("**ğŸ™ï¸ ìƒìœ„ ì–¸ê¸‰ ë„ì‹œ**")
            for _, row in location_df.head(15).iterrows():
                st.markdown(f"- {row['city']}: **{row['count']}**íšŒ")
            
            # í•œêµ­ ë„ì‹œ í•˜ì´ë¼ì´íŠ¸
            korea_cities = location_df[location_df['city'].str.lower().isin(['seoul', 'busan', 'incheon', 'daegu', 'gwangju', 'daejeon', 'ulsan', 'jeju'])]
            if not korea_cities.empty:
                st.markdown("---")
                st.markdown("ğŸ‡°ğŸ‡· **í•œêµ­ ë„ì‹œ**")
                for _, row in korea_cities.iterrows():
                    st.markdown(f"- {row['city']}: **{row['count']}**íšŒ")
    else:
        st.info("í•´ë‹¹ ê¸°ê°„ì— ì§€ì—­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    st.divider()
    
    # ========== 9. AI ì—°êµ¬ ì¸ì‚¬ì´íŠ¸ ==========
    st.subheader("ğŸ¤– AI ì—°êµ¬ ì¸ì‚¬ì´íŠ¸")
    
    st.markdown("""
    Claude AIê°€ ìˆ˜ì§‘ëœ ë…¼ë¬¸ë“¤ì„ ë¶„ì„í•˜ì—¬ ì—°êµ¬ íŠ¸ë Œë“œ, ê±¸, ë– ì˜¤ë¥´ëŠ” ì§ˆë¬¸ ë“±ì„ ì œì•ˆí•©ë‹ˆë‹¤.
    """)
    
    if not ANTHROPIC_AVAILABLE:
        st.warning("Anthropic ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    elif not os.environ.get('ANTHROPIC_API_KEY'):
        st.warning("ANTHROPIC_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    else:
        if st.button("ğŸ¤– AI ì¸ì‚¬ì´íŠ¸ ìƒì„±", type="primary"):
            with st.spinner("Claudeê°€ ë…¼ë¬¸ì„ ë¶„ì„ ì¤‘..."):
                insights = generate_ai_insights(db, days, period_keywords)
                
                if insights:
                    st.markdown(insights)
                else:
                    st.error("ì¸ì‚¬ì´íŠ¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        else:
            st.info("ë²„íŠ¼ì„ í´ë¦­í•˜ë©´ Claude AIê°€ ì—°êµ¬ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")


def get_period_stats(db: DashboardDB, days: int) -> dict:
    """ê¸°ê°„ë³„ í†µê³„ ì¡°íšŒ"""
    date_from = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
    
    with db.get_connection() as conn:
        cursor = conn.cursor()
        
        # ì „ì²´ ë…¼ë¬¸ ìˆ˜
        cursor.execute("""
            SELECT COUNT(*) FROM articles 
            WHERE DATE(fetched_at) >= ?
        """, (date_from,))
        total = cursor.fetchone()[0]
        
        # High priority
        cursor.execute("""
            SELECT COUNT(*) FROM articles 
            WHERE DATE(fetched_at) >= ? AND priority = 'high'
        """, (date_from,))
        high = cursor.fetchone()[0]
        
        # Medium priority
        cursor.execute("""
            SELECT COUNT(*) FROM articles 
            WHERE DATE(fetched_at) >= ? AND priority = 'medium'
        """, (date_from,))
        medium = cursor.fetchone()[0]
        
        # ì½ìŒ
        cursor.execute("""
            SELECT COUNT(*) FROM articles 
            WHERE DATE(fetched_at) >= ? AND is_read = 1
        """, (date_from,))
        read = cursor.fetchone()[0]
        
        # ì¦ê²¨ì°¾ê¸°
        cursor.execute("""
            SELECT COUNT(*) FROM articles 
            WHERE DATE(fetched_at) >= ? AND is_starred = 1
        """, (date_from,))
        starred = cursor.fetchone()[0]
        
        return {
            'total': total,
            'high': high,
            'medium': medium,
            'read': read,
            'starred': starred
        }


def get_period_journal_stats(db: DashboardDB, days: int) -> pd.DataFrame:
    """ê¸°ê°„ë³„ ì €ë„ í†µê³„"""
    date_from = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
    
    query = """
        SELECT 
            j.name as journal,
            COUNT(*) as count,
            SUM(CASE WHEN a.priority = 'high' THEN 1 ELSE 0 END) as high,
            SUM(CASE WHEN a.priority = 'medium' THEN 1 ELSE 0 END) as medium
        FROM articles a
        LEFT JOIN journals j ON a.journal_id = j.id
        WHERE DATE(a.fetched_at) >= ?
        GROUP BY j.name
        ORDER BY count DESC
        LIMIT 20
    """
    
    with db.get_connection() as conn:
        df = pd.read_sql_query(query, conn, params=[date_from])
    
    return df


def get_keyword_daily_trend(db: DashboardDB, days: int, keywords: list) -> pd.DataFrame:
    """í‚¤ì›Œë“œë³„ ì¼ë³„ íŠ¸ë Œë“œ"""
    date_from = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
    
    query = """
        SELECT DATE(fetched_at) as date, keywords_matched
        FROM articles
        WHERE DATE(fetched_at) >= ?
          AND keywords_matched IS NOT NULL 
          AND keywords_matched != ''
    """
    
    with db.get_connection() as conn:
        df = pd.read_sql_query(query, conn, params=[date_from])
    
    if df.empty:
        return pd.DataFrame()
    
    # ì¼ë³„/í‚¤ì›Œë“œë³„ ì¹´ìš´íŠ¸
    daily_counts = {}
    
    for _, row in df.iterrows():
        date = row['date']
        kw_matched = row['keywords_matched']
        
        try:
            kw_list = json.loads(kw_matched)
            if isinstance(kw_list, list):
                for kw in kw_list:
                    kw = str(kw).strip()
                    if kw in keywords:
                        key = (date, kw)
                        daily_counts[key] = daily_counts.get(key, 0) + 1
        except:
            pass
    
    # DataFrameìœ¼ë¡œ ë³€í™˜
    result = []
    for (date, kw), count in daily_counts.items():
        result.append({'date': date, 'keyword': kw, 'count': count})
    
    return pd.DataFrame(result)


def get_keyword_cooccurrence(db: DashboardDB, days: int, min_count: int = 2) -> pd.DataFrame:
    """í‚¤ì›Œë“œ ê³µì¶œí˜„ ë°ì´í„° ì¶”ì¶œ"""
    date_from = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
    
    query = """
        SELECT keywords_matched
        FROM articles
        WHERE DATE(fetched_at) >= ?
          AND keywords_matched IS NOT NULL 
          AND keywords_matched != ''
    """
    
    with db.get_connection() as conn:
        df = pd.read_sql_query(query, conn, params=[date_from])
    
    if df.empty:
        return pd.DataFrame()
    
    # í‚¤ì›Œë“œ ìŒ ì¹´ìš´íŠ¸
    cooccurrence = {}
    
    for _, row in df.iterrows():
        kw_matched = row['keywords_matched']
        
        try:
            kw_list = json.loads(kw_matched)
            if isinstance(kw_list, list) and len(kw_list) >= 2:
                # ëª¨ë“  í‚¤ì›Œë“œ ìŒ ì¡°í•©
                kw_list = [str(kw).strip() for kw in kw_list]
                for i in range(len(kw_list)):
                    for j in range(i + 1, len(kw_list)):
                        pair = tuple(sorted([kw_list[i], kw_list[j]]))
                        cooccurrence[pair] = cooccurrence.get(pair, 0) + 1
        except:
            pass
    
    # DataFrameìœ¼ë¡œ ë³€í™˜
    result = []
    for (kw1, kw2), count in cooccurrence.items():
        if count >= min_count:
            result.append({'source': kw1, 'target': kw2, 'weight': count})
    
    return pd.DataFrame(result).sort_values('weight', ascending=False)


def render_keyword_network(cooccurrence_df: pd.DataFrame, keyword_stats: pd.DataFrame):
    """í‚¤ì›Œë“œ ê³µì¶œí˜„ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”"""
    if cooccurrence_df.empty:
        st.info("ê³µì¶œí˜„ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return
    
    # í‚¤ì›Œë“œ ìš°ì„ ìˆœìœ„ ë§¤í•‘
    priority_map = {}
    if not keyword_stats.empty:
        for _, row in keyword_stats.iterrows():
            priority_map[row['keyword']] = row.get('priority', 'normal')
    
    # NetworkX ê·¸ë˜í”„ ìƒì„±
    G = nx.Graph()
    
    # ì—£ì§€ ì¶”ê°€
    for _, row in cooccurrence_df.iterrows():
        G.add_edge(row['source'], row['target'], weight=row['weight'])
    
    # Pyvis ë„¤íŠ¸ì›Œí¬ ìƒì„±
    net = Network(height='500px', width='100%', bgcolor='#ffffff', font_color='#333333')
    net.barnes_hut(gravity=-3000, central_gravity=0.3, spring_length=100)
    
    # ë…¸ë“œ ì¶”ê°€ (ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ìƒ‰ìƒ)
    for node in G.nodes():
        priority = priority_map.get(node, 'normal')
        
        if priority == 'high':
            color = '#ff4b4b'  # ë¹¨ê°•
        elif priority == 'medium':
            color = '#ffa500'  # ì£¼í™©
        else:
            color = '#4A90D9'  # íŒŒë‘
        
        # ë…¸ë“œ í¬ê¸° = ì—°ê²° ìˆ˜
        size = 15 + G.degree(node) * 3
        
        net.add_node(node, label=node, color=color, size=size, title=f"{node}\nì—°ê²°: {G.degree(node)}ê°œ")
    
    # ì—£ì§€ ì¶”ê°€
    for edge in G.edges(data=True):
        weight = edge[2]['weight']
        net.add_edge(edge[0], edge[1], value=weight, title=f"ê³µì¶œí˜„: {weight}íšŒ")
    
    # HTML ìƒì„±
    with tempfile.NamedTemporaryFile(delete=False, suffix='.html', mode='w', encoding='utf-8') as f:
        net.save_graph(f.name)
        f.seek(0)
        html_content = open(f.name, 'r', encoding='utf-8').read()
    
    # Streamlitì— ë Œë”ë§
    components.html(html_content, height=520, scrolling=True)


# ========== ì´ë¡  ì—°ê²°ë§ ë¶„ì„ ==========
THEORISTS = [
    "Foucault", "Deleuze", "Guattari", "Lefebvre", "Harvey", "Massey", 
    "Latour", "Haraway", "Barad", "Bennett", "Agamben", "Butler",
    "Bourdieu", "Gramsci", "Marx", "Weber", "Simmel", "Sassen",
    "Castells", "Brenner", "Smith", "Jessop", "Peck", "Theodore"
]

THEORETICAL_CONCEPTS = [
    "governmentality", "biopolitics", "discipline", "panopticon",
    "assemblage", "rhizome", "deterritorialization", "becoming",
    "new materialism", "posthuman", "actor-network", "ANT",
    "right to the city", "production of space", "spatial triad",
    "territory", "sovereignty", "borders", "mobility",
    "infrastructure", "platform", "smart city", "algorithm",
    "neoliberalism", "gentrification", "displacement", "accumulation"
]


def analyze_theory_connections(db: DashboardDB, days: int) -> dict:
    """ì´ë¡ ê°€/ê°œë… ì—°ê²° ë¶„ì„"""
    date_from = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
    
    query = """
        SELECT title, abstract, keywords_matched
        FROM articles
        WHERE DATE(fetched_at) >= ?
          AND abstract IS NOT NULL 
          AND LENGTH(abstract) > 100
    """
    
    with db.get_connection() as conn:
        df = pd.read_sql_query(query, conn, params=[date_from])
    
    if df.empty:
        return {'theorists': {}, 'concepts': {}, 'connections': []}
    
    # ì´ë¡ ê°€ ë° ê°œë… ì¹´ìš´íŠ¸
    theorist_counts = {t: 0 for t in THEORISTS}
    concept_counts = {c: 0 for c in THEORETICAL_CONCEPTS}
    connections = {}  # (ì´ë¡ ê°€, ê°œë…) ìŒ
    
    for _, row in df.iterrows():
        text = f"{row['title']} {row['abstract']}".lower()
        
        found_theorists = []
        found_concepts = []
        
        for t in THEORISTS:
            if t.lower() in text:
                theorist_counts[t] += 1
                found_theorists.append(t)
        
        for c in THEORETICAL_CONCEPTS:
            if c.lower() in text:
                concept_counts[c] += 1
                found_concepts.append(c)
        
        # ì—°ê²° ê¸°ë¡
        for t in found_theorists:
            for c in found_concepts:
                key = (t, c)
                connections[key] = connections.get(key, 0) + 1
    
    # í•„í„°ë§ (0ë³´ë‹¤ í° ê²ƒë§Œ)
    theorist_counts = {k: v for k, v in theorist_counts.items() if v > 0}
    concept_counts = {k: v for k, v in concept_counts.items() if v > 0}
    connections = [(k[0], k[1], v) for k, v in connections.items() if v > 0]
    connections.sort(key=lambda x: -x[2])
    
    return {
        'theorists': theorist_counts,
        'concepts': concept_counts,
        'connections': connections[:30]  # Top 30
    }


def render_theory_network(theory_data: dict):
    """ì´ë¡  ì—°ê²°ë§ ì‹œê°í™”"""
    if not theory_data['connections']:
        st.info("ì´ë¡ ì  ì—°ê²° ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        return
    
    G = nx.Graph()
    
    # ë…¸ë“œ ì¶”ê°€
    for t, count in theory_data['theorists'].items():
        G.add_node(t, node_type='theorist', count=count)
    
    for c, count in theory_data['concepts'].items():
        G.add_node(c, node_type='concept', count=count)
    
    # ì—£ì§€ ì¶”ê°€
    for t, c, weight in theory_data['connections']:
        G.add_edge(t, c, weight=weight)
    
    # Pyvis
    net = Network(height='500px', width='100%', bgcolor='#ffffff', font_color='#333333')
    net.barnes_hut(gravity=-2000, central_gravity=0.3, spring_length=150)
    
    for node in G.nodes(data=True):
        name = node[0]
        data = node[1]
        
        if data.get('node_type') == 'theorist':
            color = '#9b59b6'  # ë³´ë¼
            shape = 'dot'
        else:
            color = '#3498db'  # íŒŒë‘
            shape = 'box'
        
        size = 15 + data.get('count', 1) * 2
        net.add_node(name, label=name, color=color, size=size, shape=shape,
                    title=f"{name}\në“±ì¥: {data.get('count', 0)}íšŒ")
    
    for edge in G.edges(data=True):
        net.add_edge(edge[0], edge[1], value=edge[2]['weight'],
                    title=f"ì—°ê²°: {edge[2]['weight']}íšŒ")
    
    with tempfile.NamedTemporaryFile(delete=False, suffix='.html', mode='w', encoding='utf-8') as f:
        net.save_graph(f.name)
        html_content = open(f.name, 'r', encoding='utf-8').read()
    
    components.html(html_content, height=520, scrolling=True)


# ========== AI ì¸ì‚¬ì´íŠ¸ ==========
def generate_ai_insights(db: DashboardDB, days: int, keyword_stats: pd.DataFrame) -> str:
    """Claude APIë¡œ AI ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
    if not ANTHROPIC_AVAILABLE:
        return None
    
    api_key = os.environ.get('ANTHROPIC_API_KEY')
    if not api_key:
        return None
    
    date_from = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
    
    # ë°ì´í„° ìˆ˜ì§‘
    query = """
        SELECT title, title_ko, abstract, priority, keywords_matched
        FROM articles
        WHERE DATE(fetched_at) >= ?
          AND priority IN ('high', 'medium')
        ORDER BY priority DESC, fetched_at DESC
        LIMIT 30
    """
    
    with db.get_connection() as conn:
        df = pd.read_sql_query(query, conn, params=[date_from])
    
    if df.empty:
        return "ë¶„ì„í•  ë…¼ë¬¸ì´ ë¶€ì¡±í•©ë‹ˆë‹¤."
    
    # ë…¼ë¬¸ ìš”ì•½ ì¤€ë¹„
    articles_summary = []
    for _, row in df.iterrows():
        title = row['title_ko'] or row['title']
        keywords = row['keywords_matched'] or ''
        articles_summary.append(f"- [{row['priority'].upper()}] {title} (í‚¤ì›Œë“œ: {keywords})")
    
    articles_text = "\n".join(articles_summary[:20])
    
    # í‚¤ì›Œë“œ í†µê³„
    if not keyword_stats.empty:
        top_keywords = keyword_stats.head(10)['keyword'].tolist()
        keywords_text = ", ".join(top_keywords)
    else:
        keywords_text = "ë°ì´í„° ì—†ìŒ"
    
    prompt = f"""
ë‹¹ì‹ ì€ ì¸ë¬¸ì§€ë¦¬í•™/ë„ì‹œì—°êµ¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ëŠ” ìµœê·¼ {days}ì¼ê°„ ìˆ˜ì§‘ëœ í•™ìˆ ë…¼ë¬¸ ëª©ë¡ì…ë‹ˆë‹¤.

**ì£¼ìš” í‚¤ì›Œë“œ**: {keywords_text}

**ë…¼ë¬¸ ëª©ë¡**:
{articles_text}

ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

1. **ì—°êµ¬ íŠ¸ë Œë“œ ìš”ì•½** (3-4ë¬¸ì¥): ì´ ê¸°ê°„ ì–´ë–¤ ì£¼ì œê°€ í™œë°œíˆ ì—°êµ¬ë˜ê³  ìˆëŠ”ì§€

2. **ì—°êµ¬ ê±¸/ê¸°íšŒ** (2-3ê°œ): ì•„ì§ ì¶©ë¶„íˆ íƒêµ¬ë˜ì§€ ì•Šì€ ì˜ì—­, ìƒˆë¡œìš´ ì—°êµ¬ ê¸°íšŒ

3. **ë– ì˜¤ë¥´ëŠ” ì—°êµ¬ ì§ˆë¬¸** (2-3ê°œ): ì´ ë…¼ë¬¸ë“¤ì—ì„œ ë°œê²¬ë˜ëŠ” ë¯¸í•´ê²° ì§ˆë¬¸ë“¤

4. **í†µì°°ì„±/ì¸ì‚¬ì´íŠ¸** (2-3ê°œ): ë…ìì—ê²Œ ë„ì›€ì´ ë  í¥ë¯¸ë¡œìš´ ë°œê²¬

ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
    
    try:
        client = Anthropic(api_key=api_key)
        response = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=1500,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.content[0].text
    except Exception as e:
        return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


# ========== í† í”½ í´ëŸ¬ìŠ¤í„°ë§ ==========
def perform_topic_clustering(db: DashboardDB, days: int, n_clusters: int = 5) -> dict:
    """ë…¼ë¬¸ ì´ˆë¡ ê¸°ë°˜ í† í”½ í´ëŸ¬ìŠ¤í„°ë§ (TF-IDF + KMeans)"""
    date_from = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
    
    query = """
        SELECT id, title, title_ko, abstract, priority
        FROM articles
        WHERE DATE(fetched_at) >= ?
          AND abstract IS NOT NULL 
          AND LENGTH(abstract) > 100
    """
    
    with db.get_connection() as conn:
        df = pd.read_sql_query(query, conn, params=[date_from])
    
    if len(df) < n_clusters:
        return {'clusters': [], 'articles': df, 'error': 'ë°ì´í„° ë¶€ì¡±'}
    
    # TF-IDF ë²¡í„°í™”
    abstracts = df['abstract'].tolist()
    
    vectorizer = TfidfVectorizer(
        max_features=1000,
        stop_words='english',
        ngram_range=(1, 2),
        min_df=2
    )
    
    try:
        tfidf_matrix = vectorizer.fit_transform(abstracts)
    except:
        return {'clusters': [], 'articles': df, 'error': 'TF-IDF ì‹¤íŒ¨'}
    
    # KMeans í´ëŸ¬ìŠ¤í„°ë§
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    df['cluster'] = kmeans.fit_predict(tfidf_matrix)
    
    # PCAë¡œ 2D ì¶•ì†Œ
    pca = PCA(n_components=2, random_state=42)
    coords = pca.fit_transform(tfidf_matrix.toarray())
    df['x'] = coords[:, 0]
    df['y'] = coords[:, 1]
    
    # í´ëŸ¬ìŠ¤í„°ë³„ ëŒ€í‘œ í‚¤ì›Œë“œ ì¶”ì¶œ
    feature_names = vectorizer.get_feature_names_out()
    cluster_keywords = {}
    
    for i in range(n_clusters):
        center = kmeans.cluster_centers_[i]
        top_indices = center.argsort()[-5:][::-1]
        cluster_keywords[i] = [feature_names[idx] for idx in top_indices]
    
    return {
        'clusters': cluster_keywords,
        'articles': df,
        'error': None
    }


# ========== ì‚¬ë¡€ ì§€ì—­ ì¶”ì¶œ ë° ì§€ë„ ==========
CITY_COORDS = {
    # ì•„ì‹œì•„
    "seoul": (37.5665, 126.9780), "tokyo": (35.6762, 139.6503), 
    "beijing": (39.9042, 116.4074), "shanghai": (31.2304, 121.4737),
    "hong kong": (22.3193, 114.1694), "singapore": (1.3521, 103.8198),
    "bangkok": (13.7563, 100.5018), "mumbai": (19.0760, 72.8777),
    "delhi": (28.7041, 77.1025), "jakarta": (-6.2088, 106.8456),
    # ìœ ëŸ½
    "london": (51.5074, -0.1278), "paris": (48.8566, 2.3522),
    "berlin": (52.5200, 13.4050), "amsterdam": (52.3676, 4.9041),
    "barcelona": (41.3851, 2.1734), "rome": (41.9028, 12.4964),
    "vienna": (48.2082, 16.3738), "copenhagen": (55.6761, 12.5683),
    "stockholm": (59.3293, 18.0686), "oslo": (59.9139, 10.7522),
    # ë¶ë¯¸
    "new york": (40.7128, -74.0060), "los angeles": (34.0522, -118.2437),
    "chicago": (41.8781, -87.6298), "san francisco": (37.7749, -122.4194),
    "toronto": (43.6532, -79.3832), "vancouver": (49.2827, -123.1207),
    "mexico city": (19.4326, -99.1332),
    # ë‚¨ë¯¸
    "sao paulo": (-23.5505, -46.6333), "buenos aires": (-34.6037, -58.3816),
    "rio de janeiro": (-22.9068, -43.1729), "bogota": (4.7110, -74.0721),
    # ì•„í”„ë¦¬ì¹´/ì¤‘ë™
    "cape town": (-33.9249, 18.4241), "johannesburg": (-26.2041, 28.0473),
    "cairo": (30.0444, 31.2357), "dubai": (25.2048, 55.2708),
    "istanbul": (41.0082, 28.9784), "tel aviv": (32.0853, 34.7818),
    # ì˜¤ì„¸ì•„ë‹ˆì•„
    "sydney": (-33.8688, 151.2093), "melbourne": (-37.8136, 144.9631),
    "auckland": (-36.8509, 174.7645),
    # í•œêµ­ ë„ì‹œ
    "busan": (35.1796, 129.0756), "incheon": (37.4563, 126.7052),
    "daegu": (35.8714, 128.6014), "gwangju": (35.1595, 126.8526),
    "daejeon": (36.3504, 127.3845), "ulsan": (35.5384, 129.3114),
    "jeju": (33.4996, 126.5312),
}


def extract_locations(db: DashboardDB, days: int) -> pd.DataFrame:
    """ë…¼ë¬¸ì—ì„œ ì§€ì—­ëª… ì¶”ì¶œ"""
    date_from = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
    
    query = """
        SELECT title, abstract
        FROM articles
        WHERE DATE(fetched_at) >= ?
          AND abstract IS NOT NULL
    """
    
    with db.get_connection() as conn:
        df = pd.read_sql_query(query, conn, params=[date_from])
    
    if df.empty:
        return pd.DataFrame()
    
    # ì§€ì—­ëª… ì¹´ìš´íŠ¸
    location_counts = {}
    
    for _, row in df.iterrows():
        text = f"{row['title']} {row['abstract']}".lower()
        
        for city in CITY_COORDS.keys():
            pattern = r'\b' + re.escape(city) + r'\b'
            if re.search(pattern, text):
                location_counts[city] = location_counts.get(city, 0) + 1
    
    result = []
    for city, count in location_counts.items():
        lat, lon = CITY_COORDS[city]
        result.append({
            'city': city.title(),
            'count': count,
            'lat': lat,
            'lon': lon
        })
    
    return pd.DataFrame(result).sort_values('count', ascending=False)


def render_location_map(location_df: pd.DataFrame):
    """ì‚¬ë¡€ ì§€ì—­ ì§€ë„ ì‹œê°í™”"""
    if not FOLIUM_AVAILABLE:
        st.warning("folium ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    if location_df.empty:
        st.info("ì§€ì—­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    m = folium.Map(location=[20, 0], zoom_start=2, tiles='cartodbpositron')
    
    max_count = location_df['count'].max()
    
    for _, row in location_df.iterrows():
        radius = 5 + (row['count'] / max_count) * 25
        
        folium.CircleMarker(
            location=[row['lat'], row['lon']],
            radius=radius,
            popup=f"{row['city']}: {row['count']}íšŒ",
            color='#ff4b4b',
            fill=True,
            fill_color='#ff4b4b',
            fill_opacity=0.6
        ).add_to(m)
    
    folium_static(m, width=700, height=400)


def render_statistics(db: DashboardDB):
    """í†µê³„ í™”ë©´"""
    st.title("ğŸ“ˆ í†µê³„")
    
    tab1, tab2, tab3 = st.tabs(["ğŸ“Š íŠ¸ë Œë“œ", "ğŸ“° ì €ë„", "ğŸ·ï¸ í‚¤ì›Œë“œ"])
    
    with tab1:
        st.subheader("ì¼ë³„ ìˆ˜ì§‘ í˜„í™©")
        
        days = st.slider("ê¸°ê°„ (ì¼)", 7, 90, 30)
        daily = db.get_daily_counts(days)
        
        if not daily.empty:
            fig = go.Figure()
            
            fig.add_trace(go.Scatter(
                x=daily['date'],
                y=daily['count'],
                mode='lines+markers',
                name='ì „ì²´',
                line=dict(color='#4A90D9', width=2)
            ))
            
            fig.add_trace(go.Bar(
                x=daily['date'],
                y=daily['high'],
                name='High',
                marker_color='#ff4b4b'
            ))
            
            fig.add_trace(go.Bar(
                x=daily['date'],
                y=daily['medium'],
                name='Medium',
                marker_color='#ffa500'
            ))
            
            fig.update_layout(
                xaxis_title="ë‚ ì§œ",
                yaxis_title="ë…¼ë¬¸ ìˆ˜",
                barmode='stack',
                height=400
            )
            
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    with tab2:
        st.subheader("ì €ë„ë³„ ë…¼ë¬¸ ìˆ˜")
        
        journal_dist = db.get_journal_distribution()
        
        if not journal_dist.empty:
            fig = px.bar(
                journal_dist,
                x='count',
                y='journal',
                orientation='h',
                color='high',
                color_continuous_scale='Reds'
            )
            fig.update_layout(
                yaxis={'categoryorder': 'total ascending'},
                height=600,
                xaxis_title="ë…¼ë¬¸ ìˆ˜",
                yaxis_title=""
            )
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    with tab3:
        st.subheader("ë§¤ì¹­ëœ í‚¤ì›Œë“œ ë¹ˆë„")
        
        keyword_stats = db.get_keyword_stats()
        
        if not keyword_stats.empty:
            fig = px.bar(
                keyword_stats.head(20),
                x='count',
                y='keyword',
                orientation='h',
                color='count',
                color_continuous_scale='Blues'
            )
            fig.update_layout(
                yaxis={'categoryorder': 'total ascending'},
                height=500,
                xaxis_title="ë¹ˆë„",
                yaxis_title=""
            )
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("í‚¤ì›Œë“œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


def render_settings():
    """ì„¤ì • í™”ë©´"""
    st.title("âš™ï¸ ì„¤ì •")
    
    config = load_config()
    
    # í‚¤ì›Œë“œ ì„¤ì • ì„¹ì…˜
    st.subheader("ğŸ·ï¸ í‚¤ì›Œë“œ ì„¤ì •")
    
    st.markdown("""
    ë…¼ë¬¸ ìš°ì„ ìˆœìœ„ ë¶„ë¥˜ì— ì‚¬ìš©ë˜ëŠ” í‚¤ì›Œë“œë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
    - **ğŸ”´ High Priority**: í•µì‹¬ ì—°êµ¬ í‚¤ì›Œë“œ (ë²ˆì—­ ë° ìš”ì•½ ëŒ€ìƒ)
    - **ğŸŸ¡ Medium Priority**: ê´€ì‹¬ í‚¤ì›Œë“œ (ë²ˆì—­ ëŒ€ìƒ)
    """)
    
    # í˜„ì¬ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
    keywords_config = config.get('keywords', {})
    high_keywords = keywords_config.get('priority_high', [])
    medium_keywords = keywords_config.get('priority_medium', [])
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### ğŸ”´ High Priority í‚¤ì›Œë“œ")
        
        # í˜„ì¬ í‚¤ì›Œë“œ í‘œì‹œ (ì‚­ì œ ê°€ëŠ¥)
        st.caption(f"í˜„ì¬ {len(high_keywords)}ê°œ")
        
        # íƒœê·¸ í˜•íƒœë¡œ í‘œì‹œ
        if high_keywords:
            cols = st.columns(3)
            for i, kw in enumerate(high_keywords):
                with cols[i % 3]:
                    if st.button(f"âŒ {kw}", key=f"del_high_{i}", help=f"'{kw}' ì‚­ì œ"):
                        high_keywords.remove(kw)
                        config['keywords']['priority_high'] = high_keywords
                        save_config(config)
                        st.rerun()
        
        # ìƒˆ í‚¤ì›Œë“œ ì¶”ê°€
        st.markdown("---")
        new_high = st.text_input("ìƒˆ High í‚¤ì›Œë“œ ì¶”ê°€", key="new_high", placeholder="í‚¤ì›Œë“œ ì…ë ¥ í›„ Enter")
        if new_high and new_high.strip():
            new_kw = new_high.strip().lower()
            if new_kw not in high_keywords:
                if st.button("â• ì¶”ê°€", key="add_high"):
                    high_keywords.append(new_kw)
                    config['keywords']['priority_high'] = high_keywords
                    save_config(config)
                    st.success(f"'{new_kw}' ì¶”ê°€ë¨!")
                    st.rerun()
            else:
                st.warning("ì´ë¯¸ ì¡´ì¬í•˜ëŠ” í‚¤ì›Œë“œì…ë‹ˆë‹¤.")
    
    with col2:
        st.markdown("#### ğŸŸ¡ Medium Priority í‚¤ì›Œë“œ")
        
        st.caption(f"í˜„ì¬ {len(medium_keywords)}ê°œ")
        
        if medium_keywords:
            cols = st.columns(3)
            for i, kw in enumerate(medium_keywords):
                with cols[i % 3]:
                    if st.button(f"âŒ {kw}", key=f"del_med_{i}", help=f"'{kw}' ì‚­ì œ"):
                        medium_keywords.remove(kw)
                        config['keywords']['priority_medium'] = medium_keywords
                        save_config(config)
                        st.rerun()
        
        st.markdown("---")
        new_medium = st.text_input("ìƒˆ Medium í‚¤ì›Œë“œ ì¶”ê°€", key="new_medium", placeholder="í‚¤ì›Œë“œ ì…ë ¥ í›„ Enter")
        if new_medium and new_medium.strip():
            new_kw = new_medium.strip().lower()
            if new_kw not in medium_keywords:
                if st.button("â• ì¶”ê°€", key="add_medium"):
                    medium_keywords.append(new_kw)
                    config['keywords']['priority_medium'] = medium_keywords
                    save_config(config)
                    st.success(f"'{new_kw}' ì¶”ê°€ë¨!")
                    st.rerun()
            else:
                st.warning("ì´ë¯¸ ì¡´ì¬í•˜ëŠ” í‚¤ì›Œë“œì…ë‹ˆë‹¤.")
    
    st.divider()
    
    # ì´ë©”ì¼ ì„¤ì •
    st.subheader("ğŸ“§ ì´ë©”ì¼ ì•Œë¦¼")
    
    col1, col2 = st.columns(2)
    with col1:
        st.text_input("ë°›ì„ ì´ë©”ì¼", value="dw.gimm@gmail.com", disabled=True)
    with col2:
        st.toggle("ì´ë©”ì¼ ì•Œë¦¼ í™œì„±í™”", value=True, disabled=True)
    
    st.caption("ğŸ’¡ ì´ë©”ì¼ ì„¤ì •ì€ GitHub Secretsì—ì„œ ê´€ë¦¬ë©ë‹ˆë‹¤.")
    
    st.divider()
    
    # ì „ì²´ ì„¤ì • ë³´ê¸° (ì ‘ê¸°)
    with st.expander("ğŸ“„ ì „ì²´ ì„¤ì • íŒŒì¼ ë³´ê¸° (config.yaml)"):
        config_path = Path("./config.yaml")
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                st.code(f.read(), language='yaml')
        else:
            st.warning("config.yamlì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
