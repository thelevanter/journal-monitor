"""
openalex.py - OpenAlex API ì—°ë™ ëª¨ë“ˆ
ì¼€ì´ì˜ í•™ìˆ ì €ë„ RSS ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

OpenAlex: ë¬´ë£Œ í•™ìˆ  ë©”íƒ€ë°ì´í„° API
- DOI ê¸°ë°˜ ë…¼ë¬¸ ì •ë³´ ì¡°íšŒ
- ì´ˆë¡ì´ ì—†ëŠ” ë…¼ë¬¸ ë³´ì¶©ì— í™œìš©
"""

import requests
import time
import logging
from typing import Dict, List, Optional
from dataclasses import dataclass

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class OpenAlexWork:
    """OpenAlex ë…¼ë¬¸ ì •ë³´ ë°ì´í„° í´ë˜ìŠ¤"""
    doi: str
    title: str
    abstract: str
    authors: List[str]
    publication_year: int
    cited_by_count: int
    open_access_url: Optional[str] = None


class OpenAlexClient:
    """OpenAlex API í´ë¼ì´ì–¸íŠ¸"""
    
    BASE_URL = "https://api.openalex.org"
    
    def __init__(self, email: str = None, request_delay: float = 0.2):
        """
        í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        
        Args:
            email: API ìš”ì²­ ì‹œ ì‚¬ìš©í•  ì´ë©”ì¼ (polite poolìš©, ì†ë„ ì œí•œ ì™„í™”)
            request_delay: ìš”ì²­ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        """
        self.email = email
        self.request_delay = request_delay
        self.session = requests.Session()
        
        # User-Agent ì„¤ì • (polite pool ì´ìš©)
        if email:
            self.session.headers.update({
                'User-Agent': f'JournalMonitor/1.0 (mailto:{email})'
            })
        else:
            self.session.headers.update({
                'User-Agent': 'JournalMonitor/1.0'
            })
    
    def _reconstruct_abstract(self, inverted_index: Dict) -> str:
        """
        OpenAlexì˜ inverted index í˜•ì‹ ì´ˆë¡ì„ ì›ë¬¸ìœ¼ë¡œ ë³µì›
        
        OpenAlexëŠ” ì´ˆë¡ì„ {word: [positions]} í˜•íƒœë¡œ ì €ì¥
        ì˜ˆ: {"The": [0], "quick": [1], "brown": [2], ...}
        
        Args:
            inverted_index: OpenAlex abstract_inverted_index í•„ë“œ
            
        Returns:
            ë³µì›ëœ ì´ˆë¡ í…ìŠ¤íŠ¸
        """
        if not inverted_index:
            return ""
        
        # ìœ„ì¹˜ â†’ ë‹¨ì–´ ë§¤í•‘
        position_word = {}
        for word, positions in inverted_index.items():
            for pos in positions:
                position_word[pos] = word
        
        # ìœ„ì¹˜ ìˆœì„œëŒ€ë¡œ ë‹¨ì–´ ì¡°í•©
        if not position_word:
            return ""
        
        max_pos = max(position_word.keys())
        words = [position_word.get(i, '') for i in range(max_pos + 1)]
        
        return ' '.join(words)
    
    def get_work_by_doi(self, doi: str) -> Optional[OpenAlexWork]:
        """
        DOIë¡œ ë…¼ë¬¸ ì •ë³´ ì¡°íšŒ
        
        Args:
            doi: ë…¼ë¬¸ DOI (10.xxxx/xxxx í˜•ì‹ ë˜ëŠ” ì „ì²´ URL)
            
        Returns:
            OpenAlexWork ë˜ëŠ” None
        """
        # DOI ì •ê·œí™”
        doi = doi.strip()
        if doi.startswith('http'):
            # https://doi.org/10.xxxx/xxxx â†’ 10.xxxx/xxxx
            doi = doi.split('doi.org/')[-1]
        
        # API í˜¸ì¶œ
        url = f"{self.BASE_URL}/works/https://doi.org/{doi}"
        
        try:
            time.sleep(self.request_delay)  # Rate limiting
            
            params = {}
            if self.email:
                params['mailto'] = self.email
            
            response = self.session.get(url, params=params, timeout=30)
            
            if response.status_code == 404:
                logger.debug(f"DOI not found in OpenAlex: {doi}")
                return None
            
            response.raise_for_status()
            data = response.json()
            
            # ì´ˆë¡ ë³µì›
            abstract = ""
            if data.get('abstract_inverted_index'):
                abstract = self._reconstruct_abstract(data['abstract_inverted_index'])
            
            # ì €ì ëª©ë¡ ì¶”ì¶œ
            authors = []
            for authorship in data.get('authorships', []):
                author = authorship.get('author', {})
                name = author.get('display_name', '')
                if name:
                    authors.append(name)
            
            # Open Access URL ì¶”ì¶œ
            oa_url = None
            if data.get('open_access', {}).get('oa_url'):
                oa_url = data['open_access']['oa_url']
            
            return OpenAlexWork(
                doi=doi,
                title=data.get('title', ''),
                abstract=abstract,
                authors=authors,
                publication_year=data.get('publication_year', 0),
                cited_by_count=data.get('cited_by_count', 0),
                open_access_url=oa_url
            )
            
        except requests.exceptions.RequestException as e:
            logger.error(f"OpenAlex API error for DOI {doi}: {e}")
            return None
        except Exception as e:
            logger.error(f"Unexpected error for DOI {doi}: {e}")
            return None
    
    def batch_get_abstracts(self, dois: List[str], 
                           progress_callback=None) -> Dict[str, str]:
        """
        ì—¬ëŸ¬ DOIì˜ ì´ˆë¡ ì¼ê´„ ì¡°íšŒ
        
        Args:
            dois: DOI ë¦¬ìŠ¤íŠ¸
            progress_callback: ì§„í–‰ìƒí™© ì½œë°± (current, total)
            
        Returns:
            {doi: abstract} ë”•ì…”ë„ˆë¦¬
        """
        results = {}
        total = len(dois)
        success = 0
        
        for i, doi in enumerate(dois, 1):
            work = self.get_work_by_doi(doi)
            
            if work and work.abstract:
                results[doi] = work.abstract
                success += 1
                logger.info(f"[{i}/{total}] âœ“ {doi[:40]}...")
            else:
                logger.info(f"[{i}/{total}] âœ— {doi[:40]}... (ì´ˆë¡ ì—†ìŒ)")
            
            if progress_callback:
                progress_callback(i, total)
        
        logger.info(f"ì™„ë£Œ: {success}/{total} ë…¼ë¬¸ ì´ˆë¡ íšë“")
        return results
    
    def get_work_metadata(self, doi: str) -> Optional[Dict]:
        """
        DOIë¡œ ì „ì²´ ë©”íƒ€ë°ì´í„° ì¡°íšŒ (ë””ë²„ê¹…/ë¶„ì„ìš©)
        
        Returns:
            ì›ë³¸ API ì‘ë‹µ ë”•ì…”ë„ˆë¦¬
        """
        doi = doi.strip()
        if doi.startswith('http'):
            doi = doi.split('doi.org/')[-1]
        
        url = f"{self.BASE_URL}/works/https://doi.org/{doi}"
        
        try:
            time.sleep(self.request_delay)
            
            params = {}
            if self.email:
                params['mailto'] = self.email
            
            response = self.session.get(url, params=params, timeout=30)
            response.raise_for_status()
            
            return response.json()
            
        except Exception as e:
            logger.error(f"Error fetching metadata: {e}")
            return None


def fetch_missing_abstracts(db, email: str = None, limit: int = 50, 
                           translate: bool = False, summarizer=None):
    """
    ì´ˆë¡ì´ ì—†ëŠ” ë…¼ë¬¸ë“¤ì˜ ì´ˆë¡ì„ OpenAlexì—ì„œ ê°€ì ¸ì™€ DB ì—…ë°ì´íŠ¸
    
    Args:
        db: Database ì¸ìŠ¤í„´ìŠ¤
        email: OpenAlex polite poolìš© ì´ë©”ì¼
        limit: ì²˜ë¦¬í•  ìµœëŒ€ ë…¼ë¬¸ ìˆ˜
        translate: ë²ˆì—­ ì—¬ë¶€ (summarizer í•„ìš”)
        summarizer: Summarizer ì¸ìŠ¤í„´ìŠ¤ (ë²ˆì—­ ì‹œ í•„ìš”)
        
    Returns:
        ì—…ë°ì´íŠ¸ëœ ë…¼ë¬¸ ìˆ˜
    """
    # ì´ˆë¡ ì—†ëŠ” ë…¼ë¬¸ ì¡°íšŒ
    articles = db.get_articles_without_abstract(limit=limit)
    
    if not articles:
        logger.info("ì´ˆë¡ì´ ì—†ëŠ” ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return 0
    
    logger.info(f"ì´ˆë¡ ë³´ì¶© ëŒ€ìƒ: {len(articles)}í¸")
    
    # OpenAlex í´ë¼ì´ì–¸íŠ¸
    client = OpenAlexClient(email=email)
    
    updated = 0
    
    for i, article in enumerate(articles, 1):
        doi = article.get('doi')
        if not doi:
            continue
        
        logger.info(f"[{i}/{len(articles)}] {article.get('title', '')[:50]}...")
        
        # OpenAlexì—ì„œ ì´ˆë¡ ê°€ì ¸ì˜¤ê¸°
        work = client.get_work_by_doi(doi)
        
        if work and work.abstract and len(work.abstract) >= 50:
            if translate and summarizer:
                # ë²ˆì—­ë„ í•¨ê»˜ ìˆ˜í–‰
                try:
                    temp_article = {
                        'title': article.get('title', ''),
                        'abstract': work.abstract
                    }
                    result = summarizer.translate_and_summarize(temp_article)
                    
                    db.update_article_abstract(
                        article['id'],
                        work.abstract,
                        result.abstract_ko,
                        result.summary_ko
                    )
                    
                    # ìš°ì„ ìˆœìœ„ë„ ì—…ë°ì´íŠ¸
                    if result.priority != 'normal':
                        db.update_article_priority(
                            article['id'],
                            result.priority,
                            result.keywords_matched
                        )
                    
                    logger.info(f"  â†’ ì´ˆë¡ + ë²ˆì—­ ì™„ë£Œ (ìš°ì„ ìˆœìœ„: {result.priority})")
                    
                except Exception as e:
                    logger.error(f"  â†’ ë²ˆì—­ ì‹¤íŒ¨: {e}")
                    db.update_article_abstract(article['id'], work.abstract)
            else:
                db.update_article_abstract(article['id'], work.abstract)
                logger.info(f"  â†’ ì´ˆë¡ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            
            updated += 1
        else:
            logger.info(f"  â†’ OpenAlexì— ì´ˆë¡ ì—†ìŒ")
    
    logger.info(f"\nì´ {updated}í¸ ì´ˆë¡ ë³´ì¶© ì™„ë£Œ")
    return updated


def recheck_priorities(db, summarizer):
    """
    ì´ˆë¡ì´ ìˆì§€ë§Œ í‚¤ì›Œë“œ ë§¤ì¹­ ì•ˆ ëœ ë…¼ë¬¸ë“¤ì˜ ìš°ì„ ìˆœìœ„ ì¬ê³„ì‚°
    
    Args:
        db: Database ì¸ìŠ¤í„´ìŠ¤
        summarizer: Summarizer ì¸ìŠ¤í„´ìŠ¤
        
    Returns:
        (ì¬ë¶„ë¥˜ëœ ìˆ˜, high ìˆ˜, medium ìˆ˜)
    """
    import sqlite3
    import json
    
    with sqlite3.connect(db.db_path) as conn:
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # ì´ˆë¡ì€ ìˆì§€ë§Œ keywords_matchedê°€ ì—†ëŠ” ë…¼ë¬¸ ì¡°íšŒ
        cursor.execute("""
            SELECT id, title, abstract, priority, keywords_matched
            FROM articles
            WHERE abstract IS NOT NULL 
              AND LENGTH(abstract) >= 50
              AND (keywords_matched IS NULL OR keywords_matched = '[]')
        """)
        
        articles = cursor.fetchall()
    
    if not articles:
        logger.info("ìš°ì„ ìˆœìœ„ ì¬ê³„ì‚° ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
        return 0, 0, 0
    
    logger.info(f"ìš°ì„ ìˆœìœ„ ì¬ê³„ì‚° ëŒ€ìƒ: {len(articles)}í¸")
    
    rechecked = 0
    high_count = 0
    medium_count = 0
    
    for article in articles:
        title = article['title'] or ''
        abstract = article['abstract'] or ''
        
        priority, keywords = summarizer._check_priority(title, abstract)
        
        if keywords:  # í‚¤ì›Œë“œ ë§¤ì¹­ëœ ê²½ìš°ë§Œ ì—…ë°ì´íŠ¸
            db.update_article_priority(article['id'], priority, keywords)
            rechecked += 1
            
            if priority == 'high':
                high_count += 1
                logger.info(f"  ğŸ”´ HIGH: {title[:50]}... â†’ {keywords}")
            elif priority == 'medium':
                medium_count += 1
                logger.info(f"  ğŸŸ¡ MEDIUM: {title[:50]}... â†’ {keywords}")
    
    logger.info(f"\nì¬ë¶„ë¥˜ ì™„ë£Œ: {rechecked}í¸ (ğŸ”´ {high_count} / ğŸŸ¡ {medium_count})")
    return rechecked, high_count, medium_count


def translate_priority_articles(db, summarizer, priorities=['high', 'medium']):
    """
    íŠ¹ì • ìš°ì„ ìˆœìœ„ ë…¼ë¬¸ ì¤‘ ë²ˆì—­ ì•ˆ ëœ ê²ƒë§Œ ë²ˆì—­
    
    Args:
        db: Database ì¸ìŠ¤í„´ìŠ¤
        summarizer: Summarizer ì¸ìŠ¤í„´ìŠ¤  
        priorities: ë²ˆì—­í•  ìš°ì„ ìˆœìœ„ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ë²ˆì—­ëœ ë…¼ë¬¸ ìˆ˜
    """
    import sqlite3
    
    placeholders = ','.join(['?' for _ in priorities])
    
    with sqlite3.connect(db.db_path) as conn:
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # í•´ë‹¹ ìš°ì„ ìˆœìœ„ ì¤‘ ë²ˆì—­ ì•ˆ ëœ ë…¼ë¬¸
        cursor.execute(f"""
            SELECT id, title, abstract, priority
            FROM articles
            WHERE priority IN ({placeholders})
              AND abstract IS NOT NULL
              AND LENGTH(abstract) >= 50
              AND (abstract_ko IS NULL OR abstract_ko = '' OR summary_ko IS NULL OR summary_ko = '')
        """, priorities)
        
        articles = [dict(row) for row in cursor.fetchall()]
    
    if not articles:
        logger.info("ë²ˆì—­í•  ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return 0
    
    logger.info(f"ë²ˆì—­ ëŒ€ìƒ: {len(articles)}í¸ (ìš°ì„ ìˆœìœ„: {', '.join(priorities)})")
    
    translated = 0
    
    for i, article in enumerate(articles, 1):
        logger.info(f"[{i}/{len(articles)}] {article['title'][:50]}...")
        
        try:
            result = summarizer.translate_and_summarize(article)
            
            db.update_article_translation(
                article['id'],
                result.title_ko,
                result.abstract_ko,
                result.summary_ko
            )
            
            translated += 1
            logger.info(f"  â†’ ë²ˆì—­ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"  â†’ ë²ˆì—­ ì‹¤íŒ¨: {e}")
    
    logger.info(f"\në²ˆì—­ ì™„ë£Œ: {translated}/{len(articles)}í¸")
    return translated


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    client = OpenAlexClient()
    
    # í…ŒìŠ¤íŠ¸ DOI
    test_doi = "10.1177/0309132520925833"  # Progress in Human Geography ë…¼ë¬¸
    
    print(f"Testing DOI: {test_doi}")
    work = client.get_work_by_doi(test_doi)
    
    if work:
        print(f"\nì œëª©: {work.title}")
        print(f"ì €ì: {', '.join(work.authors[:3])}{'...' if len(work.authors) > 3 else ''}")
        print(f"ì—°ë„: {work.publication_year}")
        print(f"ì¸ìš©ìˆ˜: {work.cited_by_count}")
        print(f"\nì´ˆë¡ ({len(work.abstract)}ì):")
        print(work.abstract[:300] + "..." if len(work.abstract) > 300 else work.abstract)
    else:
        print("ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
